{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85477a-f4a9-41ae-9ac1-72010bac0884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\") #honestly idek what this is but seems like I have to do it\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "import urllib.request as libreq\n",
    "from training_helpers import *\n",
    "from generate import Generator\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, \\\n",
    "Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, PegasusModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79295b-8932-4aab-83d7-0ab3c2ab2caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7649e-ec0c-4a66-b288-1e1df966437e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BUILDING DATASETS\n",
    "\n",
    "def make_arxiv_dataset():\n",
    "    start = time.time()\n",
    "    sample_indices = np.random.randint(low = 0, high = len(metadata), size = 10)\n",
    "    sampled_articles = [\"\" for _ in range(sample_size)]\n",
    "    sampled_ids = [\"\" for _ in range(sample_size)]\n",
    "    for ii, i in enumerate(sample_indices):\n",
    "        article_id = metadata[\"id\"][i]\n",
    "        sampled_ids[ii] = article_id\n",
    "        b = req_article(article_id)\n",
    "        sampled_articles[ii] = b.decode()\n",
    "    end = time.time()\n",
    "\n",
    "    save_article_list(sampled_articles, \"all_articles.txt\")\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    \n",
    "def make_wiki_dataset():\n",
    "    #wiki sentences into lists\n",
    "    with open(NORMAL_WIKI_PATH, \"rt\") as f:\n",
    "        normal = f.readlines()\n",
    "    with open(SIMPLE_WIKI_PATH, \"rt\") as f:\n",
    "        simple = f.readlines()\n",
    "    #collect titles and articles\n",
    "    normal_ds = {}\n",
    "    curr_sentences = []\n",
    "    curr_title = normal[0].split(\"\\t\")[0] #initialize with first title\n",
    "    for a in normal:\n",
    "        split = a.split(\"\\t\")\n",
    "        title, para, sentence = split[0], split[1], split[2][:-1] #drop new line character\n",
    "\n",
    "        if title != curr_title:\n",
    "            art = \" \".join(curr_sentences)\n",
    "            normal_ds[curr_title] = art\n",
    "            curr_title = title\n",
    "            curr_sentences = []        \n",
    "        curr_sentences.append(sentence)\n",
    "\n",
    "    art = \" \".join(curr_sentences)\n",
    "    normal_ds[curr_title] = art\n",
    "\n",
    "\n",
    "\n",
    "    #simple wiki\n",
    "    simple_ds = {}\n",
    "    curr_sentences = []\n",
    "    curr_title = simple[0].split(\"\\t\")[0] #initialize with first title\n",
    "    for a in simple:\n",
    "        split = a.split(\"\\t\")\n",
    "        title, para, sentence = split[0], split[1], split[2][:-1] #drop new line character\n",
    "\n",
    "        if title != curr_title:\n",
    "            art = \" \".join(curr_sentences)\n",
    "            simple_ds[curr_title] = art\n",
    "            curr_title = title\n",
    "            curr_sentences = []\n",
    "        curr_sentences.append(sentence)\n",
    "\n",
    "    art = \" \".join(curr_sentences)\n",
    "    simple_ds[curr_title] = art\n",
    "\n",
    "    titles = []\n",
    "    s_articles = []\n",
    "    n_articles = []\n",
    "    i = 0\n",
    "    for k in normal_ds.keys():\n",
    "        try:\n",
    "            s_articles.append(simple_ds[k])\n",
    "            n_articles.append(normal_ds[k])\n",
    "            titles.append(k)\n",
    "        except: \n",
    "            print(\"error thrown, probably a non-common key\")\n",
    "    ds = {\"titles\": titles, \"simple_articles\": s_articles, \"normal_articles\": n_articles}\n",
    "    hf_ds = Dataset.from_dict(ds)\n",
    "    hf_ds.save_to_disk(\"../res/datasets/wiki_aligned/aligned_wiki_ds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b164e-1bc3-4021-b223-1772bb12c2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#HELPERS\n",
    "\n",
    "def freeze_base_model_weights(model):\n",
    "    sd = model.state_dict()\n",
    "    grad_layers = [\"lm_head.weight\"]\n",
    "    c=0\n",
    "    for k in sd.keys():\n",
    "        p = sd[k]\n",
    "        if k in grad_layers:\n",
    "            p.requires_grad = True\n",
    "        else: \n",
    "            c+=1\n",
    "            \n",
    "            p.requires_grad = False\n",
    "def process_arxiv_src(path, out_dir, article_ids):\n",
    "    with open(path, \"wt\") as f:\n",
    "        pass\n",
    "    \n",
    "def save_article_list(sampled_articles,out_path):\n",
    "    with open(out_path, \"wt\") as f:\n",
    "        f.writelines(sampled_articles)\n",
    "\n",
    "        \n",
    "def req_article(article_id):\n",
    "    link = 'http://zzz.cs.cornell.edu:8080/txt_tex?' + article_id\n",
    "    with libreq.urlopen(link) as url:\n",
    "      r = url.read()\n",
    "    return r\n",
    "\n",
    "\n",
    "def text_to_inputs(ds, tokenizer, text_column, label_column):\n",
    "    def preprocess_text(batch):\n",
    "        text_encoding=tokenizer(''.join(batch[text_column]), max_length=1024)\n",
    "        batch_inputs = text_encoding[\"input_ids\"]\n",
    "        batch_attentions = text_encoding[\"attention_mask\"]\n",
    "        \n",
    "        labels_encoding=tokenizer(''.join(batch[label_column]), max_length=512)\n",
    "        batch_labels = labels_encoding[\"input_ids\"]\n",
    "        return {\"input_ids\": batch_inputs, \"attention_mask\": batch_attentions, \"labels\": batch_labels}\n",
    "    \n",
    "    return ds.map(preprocess_text, num_proc=4)\n",
    "\n",
    "\n",
    "\n",
    "def get_text_and_id(arxiv_dir, sample_size=0):\n",
    "    file_names = []\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(arxiv_dir, topdown=False):\n",
    "        print(root, dirs, files)\n",
    "        if root == arxiv_dir:\n",
    "            for name in files:\n",
    "                file_names.append(name[:-4])\n",
    "                file_paths.append(os.path.join(arxiv_dir,name))\n",
    "\n",
    "    if sample_size:\n",
    "        sample_indices = np.random.randint(low = 0, high = len(file_names), size = sample_size)   \n",
    "    else:\n",
    "        sample_indices = range(len(file_names))\n",
    "    txt_dataset = {\"id\": [], \"text\": []}\n",
    "    for i in sample_indices:\n",
    "        name = file_names[i]\n",
    "        path = file_paths[i]\n",
    "        with open(path, \"r\") as f:\n",
    "            article = f.read()\n",
    "            txt_dataset[\"id\"].append(name)\n",
    "            txt_dataset[\"text\"].append(article)\n",
    "            \n",
    "    return datasets.Dataset.from_dict(txt_dataset)\n",
    "\n",
    "\n",
    "def save_list(strings, out_path):\n",
    "    strings = [strings] if isinstance(strings, str) else strings\n",
    "    with open(out_path, \"wt\") as f:\n",
    "        for i in strings:\n",
    "            f.write(i + \"\\n\\n\\n\")\n",
    "            \n",
    "\n",
    "class ArxivWikiTrainer(Seq2SeqTrainer):\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        m = dataset.load_metric(\"bertscore\")\n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        print(\"here\")\n",
    "        return {\"accuracy\": metric_acc.compute(predictions=preds, references=p.label_ids)[\"accuracy\"]}\n",
    "\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        print(loss)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \"\"\"\n",
    "                                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201ffdd-3d51-4b62-b246-0bbbec8f4fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd56f24-a2c6-4723-9948-837cb38c33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = os.getcwd()\n",
    "PATH_TO_MODELS = \"../res/models/\"\n",
    "PATH_TO_TOKENIZERS = \"../res/tokenizers\"\n",
    "MODEL_NAMES = [\"bart-base\", \"pegasus-xsum\", \"pegasus-x-base-arxiv\", \"pegasus-x-large\", \"longformer-base-4096\", \"led-large-16384-arxiv\", \"pegasus-x-base\"]\n",
    "MODEL_PATHS = {m: os.path.join(PATH_TO_MODELS, m) for m in MODEL_NAMES}\n",
    "TOKENIZER_MAPS = {\"bart-base\": \"bart-base\", \"pegasus-xsum\": \"pegasus\",  \"pegasus-x-base-arxiv\": \"pegasus\", \"pegasus-x-base\": \"pegasus\", \"pegasus-x-large\": \"pegasus\", \"longformer-base-4096\": \"longformer-base-4096\", \"led-large-16384-arxiv\": \"longformer-base-4096\"}\n",
    "TOKENIZER_PATHS = {m: os.path.join(PATH_TO_TOKENIZERS, TOKENIZER_MAPS[m]) for m in MODEL_NAMES}\n",
    "ARXIV_DATA_PATH = \"../res/datasets/arxiv_data/arxiv_data_txt/1001\"\n",
    "ARXIV_METADATA_PATH = \"../res/datasets/arxiv_data/arxiv-metadata-oai-snapshot.json\"\n",
    "WIKI_PATH = \"../res/datasets/wiki_aligned/aligned_wiki_ds\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96e08d-5b5e-4594-b073-221327c29fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17716ce3-34d3-4003-a41c-f5a9f936b39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a22c56-4fa3-40a2-8dbc-c51a9034d77e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    # process_arxiv_src(\"all_articles.txt\", ARXIV_DATA_PATH, article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96614d8c-3df5-4abe-a470-3f7c948b5551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea165677-f888-47de-a72a-c4185538f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing datasets\n",
    "metadata = datasets.load_dataset(\"json\", data_files=ARXIV_METADATA_PATH, split=\"train\") #load abstracts\n",
    "text_and_id = get_text_and_id(ARXIV_DATA_PATH) #raw text ds w/ article ids\n",
    "\n",
    "labels = [\"\" for _ in range(len(text_and_id))]\n",
    "abstracts = metadata.select_columns([\"abstract\", \"id\"])\n",
    "abstracts = {i: abstract for i, abstract in zip(abstracts[\"id\"], abstracts[\"abstract\"])}\n",
    "for idx, article_id in enumerate(text_and_id[\"id\"]):\n",
    "    labels[idx] = abstracts[article_id]\n",
    "\n",
    "text_and_id = text_and_id.add_column(\"labels\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287e342-181d-4c8b-8c9f-3c6eca22e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056915f-e242-4b06-a641-ae27a639368f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run training on all models\n",
    "# TODO: clean up all of the stuff with the splits\n",
    "MODEL_NAMES = [\"\"]\n",
    "\n",
    "for name in MODEL_NAMES:\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATHS[name], local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATHS[name], local_files_only=True).to(device)\n",
    "    # pipe = transformers.pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    \n",
    "    #data\n",
    "    ds = text_to_inputs(text_and_id, tokenizer, \"text\", \"labels\").remove_columns([\"id\", \"text\"])\n",
    "    print(ds.features)\n",
    "    splits = ds.train_test_split(test_size=0.1)\n",
    "    train_ds = splits[\"train\"]\n",
    "    eval_ds = splits[\"test\"]\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "    \n",
    "    #prep model\n",
    "    freeze_base_model_weights(model)\n",
    "    model.enable_input_require_grads()\n",
    "    model.train()\n",
    "    \n",
    "    #trainer\n",
    "    train_arg_dict = {\"output_dir\": \"../out/\" + str(name) + \"/\", \"save_steps\": 100, \"evaluation_strategy\": \"steps\",\\\n",
    "                      \"eval_steps\": 100, \"logging_steps\": 50,  \"max_steps\": 600, \"per_device_train_batch_size\": 1}\n",
    "    train_args = Seq2SeqTrainingArguments(**train_arg_dict)\n",
    "    trainer = ArxivWikiTrainer(model, args=train_args, train_dataset=train_ds, eval_dataset = eval_ds, tokenizer=tokenizer, data_collator=data_collator)\n",
    "    trainer.train()\n",
    "    \n",
    "    del tokenizer, model, ds, splits, train_ds, eval_ds, data_collator, trainer #something causing data leak, not sure what (although this line doenst even help...)\n",
    "    torch.cuda.empty_cache()\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d6909-5a94-47cb-8e98-6cb9e3cfbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949c553-f56e-49e4-81dd-90dbc474baa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218a026-fe0d-484c-a148-d25298d56a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4976bc1-bbff-4ace-aad0-cebfc848014d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac653cb-2eb9-4cc1-8b44-6e3b5c7d2649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train#run training on all models\n",
    "#TODO: clean up all of the stuff with the splits\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATHS[\"pegasus-x-base\"], local_files_only=True).to(\"cuda:0\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATHS[\"pegasus-x-base\"], local_files_only=True)\n",
    "#data\n",
    "wiki_ds = datasets.load_from_disk(WIKI_PATH)\n",
    "ds = text_to_inputs(wiki_ds, tokenizer, \"normal_articles\", \"simple_articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6e3fa-f00f-4564-94bc-b3c142e69b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for wiki, basically the same as arxiv\n",
    "splits = ds.train_test_split(test_size=0.1)\n",
    "train_ds = splits[\"train\"]\n",
    "eval_ds = splits[\"test\"].select([i for i in range(20)])\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "\n",
    "#prep model\n",
    "freeze_base_model_weights(model)\n",
    "model.enable_input_require_grads()\n",
    "model.train()\n",
    "\n",
    "#trainer\n",
    "train_arg_dict = {\"output_dir\": \"../out/wiki-pegasus-x-base\"  + \"/\", \"save_steps\": 100, \"evaluation_strategy\": \"steps\",\\\n",
    "                  \"eval_steps\": 25, \"logging_steps\": 50,  \"max_steps\": 500, \"per_device_train_batch_size\": 1}\n",
    "train_args = Seq2SeqTrainingArguments(**train_arg_dict)\n",
    "trainer = ArxivWikiTrainer(model, args=train_args, train_dataset=train_ds, eval_dataset = eval_ds, tokenizer=tokenizer, data_collator=data_collator)\n",
    "trainer.train()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596d8f3-254c-4dd6-a3c2-9af8fa0563d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ce2aa-3fc4-4a00-ba54-e979dd00d261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2504a6-84a2-4168-bbc5-71a95f8b2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS BENCHMARKING CODE\n",
    "\n",
    "\n",
    "def get_losses(model_names):\n",
    "    losses = []\n",
    "    timesteps = []\n",
    "    eval_losses = []\n",
    "    eval_timesteps = []\n",
    "    for m in model_names:\n",
    "        losses_model = []\n",
    "        timesteps_model = []\n",
    "        eval_losses_model = []\n",
    "        eval_timesteps_model = []\n",
    "        path = os.path.join(PATH_TO_CHECKPOINTS, m)\n",
    "        newest = 0 #most recent checkpoint has logs from every step\n",
    "        for d in os.listdir(path):\n",
    "            if d[0: 10] == \"checkpoint\":\n",
    "                newest = max(newest, int(d[11:])) #index goes from 10 to 11 because skipping the \"-\"\n",
    "                \n",
    "        newest_path = os.path.join(path, \"checkpoint-\" + str(newest) + \"/trainer_state.json\")\n",
    "        with open(newest_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "            #each log is a dict\n",
    "            for log in state[\"log_history\"]:\n",
    "                if \"eval_loss\" in log.keys():\n",
    "                    eval_losses_model.append(log[\"eval_loss\"])\n",
    "                    eval_timesteps_model.append(log[\"step\"])\n",
    "                else:\n",
    "                    losses_model.append(log[\"loss\"])\n",
    "                    timesteps_model.append(log[\"step\"])\n",
    "        \n",
    "        \n",
    "        losses.append(losses_model)\n",
    "        timesteps.append(timesteps_model)\n",
    "        eval_losses.append(eval_losses_model)\n",
    "        eval_timesteps.append(eval_timesteps_model)\n",
    "\n",
    "    return losses, timesteps, eval_losses, eval_timesteps      \n",
    "                    \n",
    "\n",
    "\n",
    "def plot_losses(losses, timesteps, model_names, metric_name):\n",
    "    colors = iter(plt.cm.viridis(np.linspace(0, 1, len(losses))))\n",
    "    for l, t, c, n in zip(losses, timesteps, colors, model_names):\n",
    "        plt.plot(t, l, '.', color=c)\n",
    "        plt.plot(t, l, alpha=1, color=c, label=n)\n",
    "    \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"Batches\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(metric_name)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def save_list(strings, out_path):\n",
    "    strings = [strings] if isinstance(strings, str) else strings\n",
    "    with open(out_path, \"wt\") as f:\n",
    "        for i in strings:\n",
    "            f.write(i + \"\\n\\n\\n\")\n",
    "          \n",
    "        \n",
    "def save_model_outs(articles, model, tokenizer, out_path):\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    pipe = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=-1)\n",
    "    summs = [d[\"summary_text\"] for d in pipe(articles)]\n",
    "    save_list(summs, out_path)\n",
    "\n",
    "#TODO: LOAD FROM CHECKPOINT FOR AFTER\n",
    "#given list of articles, save before and after\n",
    "def before_and_after(articles, model_name):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATHS[model_name], local_files_only=True)\n",
    "    before_model = AutoModelForSeq2SeqLM.from_pretrained(UNTRAINED_PATHS[model_name], local_files_only=True)\n",
    "    freeze_weights(before_model)\n",
    "    before_path = model_name + \"before.txt\"\n",
    "    save_model_outs(articles, before_model, tokenizer, before_path)\n",
    "\n",
    "    after_model = AutoModelForSeq2SeqLM.from_pretrained(TRAINED_PATHS[model_name], local_files_only=True)\n",
    "    freeze_weights(after_model)\n",
    "    after_path = model_name + \"_after.txt\"            \n",
    "    save_model_outs(articles, after_model, tokenizer, after_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca498f-4bbc-41ac-bc4f-703054edc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS THE CODE FOR THE EMBEDDINGS VISUALIZATION\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import enchant\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def save_json(d, out_path):\n",
    "    with open(out_path, \"wt\") as f:\n",
    "        json.dump(d, f)\n",
    "\n",
    "\n",
    "def tokenize_and_embed(words, layer):\n",
    "    global tokenizer_time\n",
    "    global inference_time\n",
    "    global use_pooler\n",
    "    global device\n",
    "\n",
    "    start = time.time()\n",
    "    tokens = tokenizer(words, return_tensors=\"pt\", padding=True).to(device)\n",
    "    end = time.time()\n",
    "    tokenizer_time += end - start\n",
    "\n",
    "    start = time.time()\n",
    "    # shape is [batch size, sequence length, model dim]\n",
    "    outs = model(**tokens,\n",
    "                 output_hidden_states=True)  # all hidden layers and initial embedding layer. outs[\"hidden_states\"][-1] == outs[\"last_hidden_state\"]\n",
    "    if use_pooler:\n",
    "        embeddings = outs[\"pooler_output\"]\n",
    "        out = embeddings.cpu().numpy()  # pooler is always correct dim, don't need to do anything else\n",
    "    else:\n",
    "        embeddings = outs[\"hidden_states\"][layer][:, 1:-1]  # choose hidden state, and then drop start and end tokens\n",
    "        out = (torch.mean(embeddings, 1)).squeeze().cpu().numpy()\n",
    "    end = time.time()\n",
    "    inference_time += end - start\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_vocab(out_path, max_vocab_size, only_alpha=True):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    vocab = {}\n",
    "    n_words = 0\n",
    "    with open(\"vocab.txt\", \"rt\", errors=\"replace\") as f:\n",
    "        for line in f:\n",
    "            if n_words == max_vocab_size:\n",
    "                break\n",
    "            else:\n",
    "                split = line.split()\n",
    "                word, count = split[0], split[1]\n",
    "                if only_alpha:\n",
    "                    if word.isalpha() and d.check(word):\n",
    "                        vocab[word] = count\n",
    "                        n_words += 1\n",
    "                else:\n",
    "                    vocab[word] = count\n",
    "                    n_words += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# take vocab dict and return dict with dim reduced embeddings\n",
    "def make_embeddings(vocab, embedder, layer, batch_size=1):\n",
    "    global sklearn_time\n",
    "    n_words = len(vocab)\n",
    "    embeddings = np.zeros((n_words, embedding_dim))\n",
    "    words = [\"\" for _ in range(batch_size)]\n",
    "    for i, k in enumerate(vocab.keys()):\n",
    "        words[i % batch_size] = k\n",
    "        nth_word = i + 1\n",
    "        if (nth_word) % batch_size == 0:  # for now just assume batch_size divides vocab\n",
    "            e = embedder(words, layer)  # e is numpy array with embeddings for words\n",
    "            embeddings[nth_word - batch_size: nth_word] = e\n",
    "\n",
    "    start = time.time()\n",
    "    pca_embeddings = PCA(n_components=pca_components).fit_transform(embeddings)\n",
    "    tsne_embeddings = TSNE(n_components=tsne_components).fit_transform(embeddings)\n",
    "    end = time.time()\n",
    "    sklearn_time += end - start\n",
    "\n",
    "    d = {}\n",
    "    for i, w in enumerate(vocab.keys()):\n",
    "        x = tsne_embeddings[i][0]\n",
    "        y = tsne_embeddings[i][1]\n",
    "        d[w] = {\"idx\": int(i % 142), \"x\": float(x),\n",
    "                \"y\": float(y)}  # 142 is because there are 142 colors in the json file, will have to address later\n",
    "    return d\n",
    "\n",
    "\n",
    "tokenizer_time = 0\n",
    "inference_time = 0\n",
    "sklearn_time = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # model.save_pretrained(\"bert_model\")\n",
    "    # tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    # tokenizer.save_pretrained(\"bert_tokenizer\")\n",
    "    # max_vocab_size = 96000\n",
    "    # vocab_path = \"vocab_eng_\" + str(int(max_vocab_size)) + \".json\"\n",
    "    # vocab_json = make_vocab(vocab_path, max_vocab_size)\n",
    "    # save_json(vocab_json, vocab_path)\n",
    "\n",
    "    batch_size = 32\n",
    "    max_vocab_size = batch_size * 3000\n",
    "    pca_components = 50\n",
    "    tsne_components = 2\n",
    "    embedding_dim = 768\n",
    "    use_pooler = False\n",
    "    model_path = \"bert_model\"\n",
    "    tokenizer_path = \"bert_tokenizer\"\n",
    "    vocab_path = \"vocab_eng_\" + str(int(max_vocab_size)) + \".json\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    layers_of_interest = [0, -1, -2, -10]\n",
    "    for layer in layers_of_interest:\n",
    "            suffix = \"pooler\" if use_pooler else str(layer)\n",
    "            embedding_path = \"embeddings_\" + str(int(max_vocab_size)) + \"_\" + suffix + \".json\"\n",
    "            with torch.no_grad():\n",
    "                model = transformers.AutoModel.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "                tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "                with open(vocab_path, \"rt\") as f:\n",
    "                    vocab = json.load(f)\n",
    "                    start = time.time()\n",
    "                    embeddings = make_embeddings(vocab, tokenize_and_embed, layer, batch_size=batch_size)\n",
    "                    end = time.time()\n",
    "                    save_json(embeddings, embedding_path)\n",
    "                    print(\"Total embedding time was \" + str(end - start))\n",
    "                    print(\"sklearn time was\" + str(sklearn_time))\n",
    "                    print(\"Inference time was\" + str(inference_time))\n",
    "                    print(\"tokenization time was\" + str(tokenizer_time))\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.p3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (SageMaker Distribution v0 GPU)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:137914896644:image/sagemaker-distribution-gpu-v0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
