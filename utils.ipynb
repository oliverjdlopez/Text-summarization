{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPERS\n",
    "\n",
    "def freeze_base_model_weights(model):\n",
    "    sd = model.state_dict()\n",
    "    grad_layers = [\"lm_head.weight\"]\n",
    "    c=0\n",
    "    for k in sd.keys():\n",
    "        p = sd[k]\n",
    "        if k in grad_layers:\n",
    "            p.requires_grad = True\n",
    "        else: \n",
    "            c+=1\n",
    "            \n",
    "            p.requires_grad = False\n",
    "def process_arxiv_src(path, out_dir, article_ids):\n",
    "    with open(path, \"wt\") as f:\n",
    "        pass\n",
    "    \n",
    "def save_article_list(sampled_articles,out_path):\n",
    "    with open(out_path, \"wt\") as f:\n",
    "        f.writelines(sampled_articles)\n",
    "\n",
    "        \n",
    "def req_article(article_id):\n",
    "    link = 'http://zzz.cs.cornell.edu:8080/txt_tex?' + article_id\n",
    "    with libreq.urlopen(link) as url:\n",
    "      r = url.read()\n",
    "    return r\n",
    "\n",
    "\n",
    "def text_to_inputs(ds, tokenizer, text_column, label_column):\n",
    "    def preprocess_text(batch):\n",
    "        text_encoding=tokenizer(''.join(batch[text_column]), max_length=1024)\n",
    "        batch_inputs = text_encoding[\"input_ids\"]\n",
    "        batch_attentions = text_encoding[\"attention_mask\"]\n",
    "        \n",
    "        labels_encoding=tokenizer(''.join(batch[label_column]), max_length=512)\n",
    "        batch_labels = labels_encoding[\"input_ids\"]\n",
    "        return {\"input_ids\": batch_inputs, \"attention_mask\": batch_attentions, \"labels\": batch_labels}\n",
    "    \n",
    "    return ds.map(preprocess_text, num_proc=4)\n",
    "\n",
    "\n",
    "\n",
    "def get_text_and_id(arxiv_dir, sample_size=0):\n",
    "    file_names = []\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(arxiv_dir, topdown=False):\n",
    "        print(root, dirs, files)\n",
    "        if root == arxiv_dir:\n",
    "            for name in files:\n",
    "                file_names.append(name[:-4])\n",
    "                file_paths.append(os.path.join(arxiv_dir,name))\n",
    "\n",
    "    if sample_size:\n",
    "        sample_indices = np.random.randint(low = 0, high = len(file_names), size = sample_size)   \n",
    "    else:\n",
    "        sample_indices = range(len(file_names))\n",
    "    txt_dataset = {\"id\": [], \"text\": []}\n",
    "    for i in sample_indices:\n",
    "        name = file_names[i]\n",
    "        path = file_paths[i]\n",
    "        with open(path, \"r\") as f:\n",
    "            article = f.read()\n",
    "            txt_dataset[\"id\"].append(name)\n",
    "            txt_dataset[\"text\"].append(article)\n",
    "            \n",
    "    return datasets.Dataset.from_dict(txt_dataset)\n",
    "\n",
    "\n",
    "def save_list(strings, out_path):\n",
    "    strings = [strings] if isinstance(strings, str) else strings\n",
    "    with open(out_path, \"wt\") as f:\n",
    "        for i in strings:\n",
    "            f.write(i + \"\\n\\n\\n\")\n",
    "            \n",
    "\n",
    "class ArxivWikiTrainer(Seq2SeqTrainer):\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        m = dataset.load_metric(\"bertscore\")\n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        print(\"here\")\n",
    "        return {\"accuracy\": metric_acc.compute(predictions=preds, references=p.label_ids)[\"accuracy\"]}\n",
    "\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        print(loss)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
