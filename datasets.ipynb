{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING DATASETS\n",
    "\n",
    "def make_arxiv_dataset():\n",
    "    start = time.time()\n",
    "    sample_indices = np.random.randint(low = 0, high = len(metadata), size = 10)\n",
    "    sampled_articles = [\"\" for _ in range(sample_size)]\n",
    "    sampled_ids = [\"\" for _ in range(sample_size)]\n",
    "    for ii, i in enumerate(sample_indices):\n",
    "        article_id = metadata[\"id\"][i]\n",
    "        sampled_ids[ii] = article_id\n",
    "        b = req_article(article_id)\n",
    "        sampled_articles[ii] = b.decode()\n",
    "    end = time.time()\n",
    "\n",
    "    save_article_list(sampled_articles, \"all_articles.txt\")\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    \n",
    "def make_wiki_dataset():\n",
    "    #wiki sentences into lists\n",
    "    with open(NORMAL_WIKI_PATH, \"rt\") as f:\n",
    "        normal = f.readlines()\n",
    "    with open(SIMPLE_WIKI_PATH, \"rt\") as f:\n",
    "        simple = f.readlines()\n",
    "    #collect titles and articles\n",
    "    normal_ds = {}\n",
    "    curr_sentences = []\n",
    "    curr_title = normal[0].split(\"\\t\")[0] #initialize with first title\n",
    "    for a in normal:\n",
    "        split = a.split(\"\\t\")\n",
    "        title, para, sentence = split[0], split[1], split[2][:-1] #drop new line character\n",
    "\n",
    "        if title != curr_title:\n",
    "            art = \" \".join(curr_sentences)\n",
    "            normal_ds[curr_title] = art\n",
    "            curr_title = title\n",
    "            curr_sentences = []        \n",
    "        curr_sentences.append(sentence)\n",
    "\n",
    "    art = \" \".join(curr_sentences)\n",
    "    normal_ds[curr_title] = art\n",
    "\n",
    "\n",
    "\n",
    "    #simple wiki\n",
    "    simple_ds = {}\n",
    "    curr_sentences = []\n",
    "    curr_title = simple[0].split(\"\\t\")[0] #initialize with first title\n",
    "    for a in simple:\n",
    "        split = a.split(\"\\t\")\n",
    "        title, para, sentence = split[0], split[1], split[2][:-1] #drop new line character\n",
    "\n",
    "        if title != curr_title:\n",
    "            art = \" \".join(curr_sentences)\n",
    "            simple_ds[curr_title] = art\n",
    "            curr_title = title\n",
    "            curr_sentences = []\n",
    "        curr_sentences.append(sentence)\n",
    "\n",
    "    art = \" \".join(curr_sentences)\n",
    "    simple_ds[curr_title] = art\n",
    "\n",
    "    titles = []\n",
    "    s_articles = []\n",
    "    n_articles = []\n",
    "    i = 0\n",
    "    for k in normal_ds.keys():\n",
    "        try:\n",
    "            s_articles.append(simple_ds[k])\n",
    "            n_articles.append(normal_ds[k])\n",
    "            titles.append(k)\n",
    "        except: \n",
    "            print(\"error thrown, probably a non-common key\")\n",
    "    ds = {\"titles\": titles, \"simple_articles\": s_articles, \"normal_articles\": n_articles}\n",
    "    hf_ds = Dataset.from_dict(ds)\n",
    "    hf_ds.save_to_disk(\"../res/datasets/wiki_aligned/aligned_wiki_ds\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
