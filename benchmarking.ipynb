{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(model_names):\n",
    "    losses = []\n",
    "    timesteps = []\n",
    "    eval_losses = []\n",
    "    eval_timesteps = []\n",
    "    for m in model_names:\n",
    "        losses_model = []\n",
    "        timesteps_model = []\n",
    "        eval_losses_model = []\n",
    "        eval_timesteps_model = []\n",
    "        path = os.path.join(PATH_TO_CHECKPOINTS, m)\n",
    "        newest = 0 #most recent checkpoint has logs from every step\n",
    "        for d in os.listdir(path):\n",
    "            if d[0: 10] == \"checkpoint\":\n",
    "                newest = max(newest, int(d[11:])) #index goes from 10 to 11 because skipping the \"-\"\n",
    "                \n",
    "        newest_path = os.path.join(path, \"checkpoint-\" + str(newest) + \"/trainer_state.json\")\n",
    "        with open(newest_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "            #each log is a dict\n",
    "            for log in state[\"log_history\"]:\n",
    "                if \"eval_loss\" in log.keys():\n",
    "                    eval_losses_model.append(log[\"eval_loss\"])\n",
    "                    eval_timesteps_model.append(log[\"step\"])\n",
    "                else:\n",
    "                    losses_model.append(log[\"loss\"])\n",
    "                    timesteps_model.append(log[\"step\"])\n",
    "        \n",
    "        \n",
    "        losses.append(losses_model)\n",
    "        timesteps.append(timesteps_model)\n",
    "        eval_losses.append(eval_losses_model)\n",
    "        eval_timesteps.append(eval_timesteps_model)\n",
    "\n",
    "    return losses, timesteps, eval_losses, eval_timesteps      \n",
    "                    \n",
    "\n",
    "\n",
    "def plot_losses(losses, timesteps, model_names, metric_name):\n",
    "    colors = iter(plt.cm.viridis(np.linspace(0, 1, len(losses))))\n",
    "    for l, t, c, n in zip(losses, timesteps, colors, model_names):\n",
    "        plt.plot(t, l, '.', color=c)\n",
    "        plt.plot(t, l, alpha=1, color=c, label=n)\n",
    "    \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"Batches\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(metric_name)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def save_list(strings, out_path):\n",
    "    strings = [strings] if isinstance(strings, str) else strings\n",
    "    with open(out_path, \"wt\") as f:\n",
    "        for i in strings:\n",
    "            f.write(i + \"\\n\\n\\n\")\n",
    "          \n",
    "        \n",
    "def save_model_outs(articles, model, tokenizer, out_path):\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    pipe = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=-1)\n",
    "    summs = [d[\"summary_text\"] for d in pipe(articles)]\n",
    "    save_list(summs, out_path)\n",
    "\n",
    "#TODO: LOAD FROM CHECKPOINT FOR AFTER\n",
    "#given list of articles, save before and after\n",
    "def before_and_after(articles, model_name):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATHS[model_name], local_files_only=True)\n",
    "    before_model = AutoModelForSeq2SeqLM.from_pretrained(UNTRAINED_PATHS[model_name], local_files_only=True)\n",
    "    freeze_weights(before_model)\n",
    "    before_path = model_name + \"before.txt\"\n",
    "    save_model_outs(articles, before_model, tokenizer, before_path)\n",
    "\n",
    "    after_model = AutoModelForSeq2SeqLM.from_pretrained(TRAINED_PATHS[model_name], local_files_only=True)\n",
    "    freeze_weights(after_model)\n",
    "    after_path = model_name + \"_after.txt\"            \n",
    "    save_model_outs(articles, after_model, tokenizer, after_path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
